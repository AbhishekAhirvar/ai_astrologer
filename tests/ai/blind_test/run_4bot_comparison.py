#!/usr/bin/env python3
"""
4-Bot Comparison for Blind Test
Runs the existing blind test 4 times (once for each bot) and compares results
Preserves all blind test protections
"""
import sys
import asyncio
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from blind_predictor import quick_blind_test
from dotenv import load_dotenv
import os

load_dotenv()


async def run_4bot_comparison(num_subjects: int = 2):
    """
    Run blind test with all 4 bots for comparison
    
    Args:
        num_subjects: Number of subjects to test (default: 2)
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå Error: OPENAI_API_KEY not found in .env")
        return
    
    print("\n" + "=" * 80)
    print("üîÆ 4-BOT BLIND TEST COMPARISON")
    print("=" * 80)
    print(f"\nThis will run the blind test 4 times with different bots:")
    print(f"  1. OMKAR_PRO  (Parashara Accuracy)")
    print(f"  2. OMKAR_LITE (Parashara Tokens)")
    print(f"  3. JYOTI_PRO  (KP Accuracy)")
    print(f"  4. JYOTI_LITE (KP Tokens)")
    print(f"\nSubjects: {num_subjects}")
    print(f"Total API calls: {num_subjects * 6 * 4} (subjects √ó questions √ó bots)")
    print("\n" + "=" * 80)
    
    # Run test with each bot configuration
    results = {}
    
    for i, (system, mode) in enumerate([
        ("parashara", "pro"),
        ("parashara", "lite"),
        ("kp", "pro"),
        ("kp", "lite")
    ], 1):
        is_kp = (system == "kp")
        bot_name = f"{'JYOTI' if is_kp else 'OMKAR'}_{mode.upper()}"
        
        print(f"\n{'=' * 80}")
        print(f"[{i}/4] Running with {bot_name}")
        print("=" * 80)
        
        result = await quick_blind_test(
            api_key=api_key,
            num_subjects=num_subjects,
            is_kp_mode=is_kp,
            bot_mode=mode
        )
        
        results[bot_name] = result
        
        # Small pause between bot runs
        if i < 4:
            print(f"\n‚è∏Ô∏è  Pausing 5 seconds before next bot...")
            await asyncio.sleep(5)
    
    print("\n" + "=" * 80)
    print("‚úÖ 4-BOT COMPARISON COMPLETE")
    print("=" * 80)
    print(f"\nAll 4 bots tested on {num_subjects} subjects")
    print(f"\nResults saved in: tests/ai/blind_test/results/")
    print(f"  - Each bot has its own JSON file with prefix 'predictions_<BOT_NAME>_'")
    print(f"  - HTML reports generated by evaluator.py")
    
    # Generate combined HTML report
    print("\nüìä Generating combined HTML report...")
    try:
        from create_combined_report import generate_combined_html
        from datetime import datetime
        
        results_dir = Path(__file__).parent / 'results'
        
        # Find the most recent prediction files for each bot
        prediction_files = []
        for bot_name in ['OMKAR_PRO', 'OMKAR_LITE', 'JYOTI_PRO', 'JYOTI_LITE']:
            files = list(results_dir.glob(f'predictions_{bot_name}_*.json'))
            if files:
                latest = max(files, key=lambda p: p.stat().st_mtime)
                prediction_files.append(latest)
        
        if len(prediction_files) == 4:
            output_path = results_dir / f'combined_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.html'
            ground_truth_path = Path(__file__).parent / 'data' / 'ground_truth_mapping.json'
            
            generate_combined_html(prediction_files, output_path, ground_truth_path)
            print(f"\n‚úÖ Combined HTML report: {output_path}")
        else:
            print(f"\n‚ö†Ô∏è  Could not generate combined report (found {len(prediction_files)}/4 bot files)")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Error generating combined report: {e}")
    
    return results


if __name__ == "__main__":
    # Parse command line args
    num_subjects = 2
    if len(sys.argv) > 1:
        try:
            num_subjects = int(sys.argv[1])
        except ValueError:
            print(f"Invalid argument: {sys.argv[1]}")
            print("Usage: python run_4bot_comparison.py [num_subjects]")
            sys.exit(1)
    
    asyncio.run(run_4bot_comparison(num_subjects))
